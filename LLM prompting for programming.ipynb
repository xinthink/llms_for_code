{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example of LLM prompting for programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir\n",
    "# !pip install langchain\n",
    "\n",
    "%reload_ext dotenv\n",
    "%dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check env -- PPLX endpoint: https://api.perplexity.ai local endpoint: http://localhost:6412/v1 proxy: socks5://localhost:1081\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import langchain\n",
    "# from langchain.callbacks.manager import CallbackManager\n",
    "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "# from langchain.chains import LLMChain\n",
    "\n",
    "# # Use llama.cpp\n",
    "# from langchain.llms import LlamaCpp\n",
    "\n",
    "# n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "# n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# # Make sure the model path is correct for your system!\n",
    "# # llama_model = \"/ws/llm_models/llama-2-7b-chat.q6_K.gguf\"\n",
    "# llama_model = \"/ws/llm_models/codellama-7b-instruct.Q6_K.gguf\"\n",
    "# llm = LlamaCpp(\n",
    "#     model_path=llama_model,\n",
    "#     n_gpu_layers=n_gpu_layers,\n",
    "#     n_batch=n_batch,\n",
    "#     f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "#     temperature=0.0,\n",
    "#     max_tokens=1024,\n",
    "#     n_ctx=4096,\n",
    "#     # top_p=1,\n",
    "#     streaming=True,\n",
    "#     callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "#     verbose=True,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Compatibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check env -- PPLX endpoint: https://api.perplexity.ai local endpoint: http://localhost:6412/v1 proxy: socks5://localhost:1081\n"
     ]
    }
   ],
   "source": [
    "# Config OpenAI client\n",
    "import os\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "print('Check env -- PPLX endpoint:', os.getenv(\"PPLX_API_ENDPOINT\"), \\\n",
    "    'local endpoint:', os.getenv(\"LOCAL_API_ENDPOINT\"), \\\n",
    "    'proxy:', os.getenv('HTTP_PROXY'))\n",
    "\n",
    "# Perplexity https://docs.perplexity.ai/reference/post_chat_completions\n",
    "openai.api_key = os.getenv(\"PPLX_API_KEY\")\n",
    "openai.api_base = os.getenv(\"PPLX_API_ENDPOINT\")\n",
    "model_name = \"codellama-34b-instruct\" # llama-2-70b-chat, llama-2-13b-chat, codellama-34b-instruct, and mistral-7b-instruct.\n",
    "\n",
    "# Local LLMs\n",
    "# openai.api_key = os.getenv(\"LOCAL_API_KEY\")\n",
    "# openai.api_base = os.getenv(\"LOCAL_API_ENDPOINT\")\n",
    "# model_name = \"\"\n",
    "\n",
    "# Azure\n",
    "# openai.api_type = 'azure'\n",
    "# openai.api_version = '2023-05-15' # this may change in the future\n",
    "# model_name = ''\n",
    "\n",
    "# os.environ[\"OPENAI_API_TYPE\"] = openai.api_type\n",
    "# os.environ[\"OPENAI_API_BASE\"] = openai.api_base\n",
    "# os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
    "# os.environ[\"OPENAI_API_VERSION\"] = openai.api_version\n",
    "\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=openai.api_key,\n",
    "    openai_api_base=openai.api_base,\n",
    "    model=model_name,\n",
    "    max_tokens=500,\n",
    "    temperature=0.0,\n",
    "    verbose=True,\n",
    "    streaming=True,\n",
    "    callback_manager=callback_manager,\n",
    ")\n",
    "openai.debug = True\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are an artificial intelligence assistant and you need to \"\n",
    "            \"engage in a helpful, detailed, polite conversation with a user.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Count to 100, with a comma between each number and no newlines. \"\n",
    "            \"E.g., 1, 2, 3, ...\"\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "# demo chat completion with streaming\n",
    "# response_stream = openai.ChatCompletion.create(\n",
    "#     api_base=openai.api_base,\n",
    "#     api_key=openai.api_key,\n",
    "#     model=model_name,\n",
    "#     messages=messages,\n",
    "#     stream=True,\n",
    "# )\n",
    "# for response in response_stream:\n",
    "#     print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm,\n",
    "                                         memory_key=\"history\",\n",
    "                                         return_messages=False,\n",
    "                                         )\n",
    "chat = ConversationChain(llm=llm,\n",
    "                         memory=memory,\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Self-testing Code with a LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sets the Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I am trained on a massive dataset of text from the internet and can generate human-like responses to a wide range of topics and questions. I can be used to create chatbots, virtual assistants, and other applications that require natural language understanding and generation capabilities.."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I am trained on a massive dataset of text from the internet and can generate human-like responses to a wide range of topics and questions. I can be used to create chatbots, virtual assistants, and other applications that require natural language understanding and generation capabilities..'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(\"Whoe are you?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Implement the shared view model as a Redux store slice and test it using vitest.\n",
      "2. Create a local view model component for displaying awareness information, which should include cursor position, name, and online status. Test this component using vitest.\n",
      "3. Develop an awareness info component that renders the cursor and name. This component should be implemented as a React component and tested using cypress component tests.\n",
      "4. Integrate the local view model component with the awareness info component to display the cursor position, name, and online status for remote users.\n",
      "5. Implement the shared view model interface to encapsulate the Redux actions for changing the state of the shared view model. Test this interface using vitest.\n",
      "6. Develop a view component that renders the Konva shapes on the whiteboard. This component should be implemented as a React component and tested using cypress component tests.\n",
      "7. Integrate the local view model component with the awareness info component and the view component to display the cursor position, name, and online status for remote users on the whiteboard.\n",
      "8. Implement the overall awareness layer by combining all of the above components and ensuring that it follows all of the guidance mentioned in the requirements section. Test this layer using vitest and cypress component tests.\n",
      "9. Review and refine the solution as necessary to ensure that it meets all of the requirements and is optimized for performance."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model cl100k_base.See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM prompting for programming.ipynb 单元格 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m qa(\u001b[39m\"\"\"\u001b[39;49m\u001b[39mThe current system is an online whiteboard system. Tech stack: typescript, react, redux, konvajs and react-konva. And vitest, react testing library for model, view model and related hooks, cypress component tests for view.\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mAll codes should be written in the tech stack mentioned above. Requirements should be implemented as react components in the MVVM architecture pattern.\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mThere are 2 types of view model in the system.\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m    1. Shared view model. View model that represents states shared among local and remote users.\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m    2. Local view model. View model that represents states only applicable to local user\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mHere are the common implementation strategy:\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m    1. Shared view model is implemented as Redux store slice. Tested in vitest.\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m    2. Local view model is implemented as React component props or states(by useState hook), unless for global local view model, which is also implemented as Redux store slice. Tested in vitest.\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m    3. Hooks are used as the major view helpers to retrieve data from shared view model. For most the case, it will use ‘createSelector’ and ‘useSelector’ for memorization. Tested in vitest and react testing library.\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m    4. Don’t dispatch action directly to change the states of shared view model, use an encapsulated view model interface instead. In the interface, each redux action is mapped to a method. Tested in vitest.\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m    5. View is consist of konva shapes, and implemented as react component via react-konva. Tested in cypress component tests\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mHere are certain patterns should be followed when implement and test the component\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m    1. When write test, use describe instead of test\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m    2. Data-driven tests are preferred.\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m    3. When test the view component, fake view model via the view model interface\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mAwareness Layer Requirement:\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mDisplay other users’ awareness info(cursor, name and online information) on the whiteboard.\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mAC1: Don’t display local user\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mAC2: When remote user changes cursor location, display the change in animation.\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mProvide an overall solution following the guidance mentioned above. Hint, keep all awareness information in a Konva layer, and an awareness info component to render cursor, and name. Don’t generate code. Describe the solution, and breaking the solution down as a task list based on the guidance mentioned above. And we will refer this task list as our master plan.\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/LLM%20prompting%20for%20programming.ipynb#X11sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/llmenv/lib/python3.11/site-packages/langchain/chains/base.py:314\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    313\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m--> 314\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprep_outputs(\n\u001b[1;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    316\u001b[0m )\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m include_run_info:\n\u001b[1;32m    318\u001b[0m     final_outputs[RUN_KEY] \u001b[39m=\u001b[39m RunInfo(run_id\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mrun_id)\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/llmenv/lib/python3.11/site-packages/langchain/chains/base.py:410\u001b[0m, in \u001b[0;36mChain.prep_outputs\u001b[0;34m(self, inputs, outputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_outputs(outputs)\n\u001b[1;32m    409\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory\u001b[39m.\u001b[39;49msave_context(inputs, outputs)\n\u001b[1;32m    411\u001b[0m \u001b[39mif\u001b[39;00m return_only_outputs:\n\u001b[1;32m    412\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/llmenv/lib/python3.11/site-packages/langchain/memory/summary_buffer.py:59\u001b[0m, in \u001b[0;36mConversationSummaryBufferMemory.save_context\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39msave_context(inputs, outputs)\n\u001b[0;32m---> 59\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprune()\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/llmenv/lib/python3.11/site-packages/langchain/memory/summary_buffer.py:64\u001b[0m, in \u001b[0;36mConversationSummaryBufferMemory.prune\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Prune buffer if it exceeds max token limit\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m buffer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_memory\u001b[39m.\u001b[39mmessages\n\u001b[0;32m---> 64\u001b[0m curr_buffer_length \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mget_num_tokens_from_messages(buffer)\n\u001b[1;32m     65\u001b[0m \u001b[39mif\u001b[39;00m curr_buffer_length \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_token_limit:\n\u001b[1;32m     66\u001b[0m     pruned_memory \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/llmenv/lib/python3.11/site-packages/langchain/chat_models/openai.py:509\u001b[0m, in \u001b[0;36mChatOpenAI.get_num_tokens_from_messages\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m    507\u001b[0m     tokens_per_name \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    508\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mget_num_tokens_from_messages() is not presently implemented \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfor model \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/openai/openai-python/blob/main/chatml.md for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minformation on how messages are converted to tokens.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m     )\n\u001b[1;32m    515\u001b[0m num_tokens \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    516\u001b[0m messages_dict \u001b[39m=\u001b[39m [convert_message_to_dict(m) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m messages]\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: get_num_tokens_from_messages() is not presently implemented for model cl100k_base.See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens."
     ]
    }
   ],
   "source": [
    "qa(\"\"\"The current system is an online whiteboard system. Tech stack: typescript, react, redux, konvajs and react-konva. And vitest, react testing library for model, view model and related hooks, cypress component tests for view.\n",
    "\n",
    "All codes should be written in the tech stack mentioned above. Requirements should be implemented as react components in the MVVM architecture pattern.\n",
    "\n",
    "There are 2 types of view model in the system.\n",
    "\n",
    "    1. Shared view model. View model that represents states shared among local and remote users.\n",
    "\n",
    "    2. Local view model. View model that represents states only applicable to local user\n",
    "\n",
    "Here are the common implementation strategy:\n",
    "\n",
    "    1. Shared view model is implemented as Redux store slice. Tested in vitest.\n",
    "\n",
    "    2. Local view model is implemented as React component props or states(by useState hook), unless for global local view model, which is also implemented as Redux store slice. Tested in vitest.\n",
    "\n",
    "    3. Hooks are used as the major view helpers to retrieve data from shared view model. For most the case, it will use ‘createSelector’ and ‘useSelector’ for memorization. Tested in vitest and react testing library.\n",
    "\n",
    "    4. Don’t dispatch action directly to change the states of shared view model, use an encapsulated view model interface instead. In the interface, each redux action is mapped to a method. Tested in vitest.\n",
    "\n",
    "    5. View is consist of konva shapes, and implemented as react component via react-konva. Tested in cypress component tests\n",
    "\n",
    "Here are certain patterns should be followed when implement and test the component\n",
    "\n",
    "    1. When write test, use describe instead of test\n",
    "\n",
    "    2. Data-driven tests are preferred.\n",
    "\n",
    "    3. When test the view component, fake view model via the view model interface\n",
    "\n",
    "Awareness Layer Requirement:\n",
    "\n",
    "Display other users’ awareness info(cursor, name and online information) on the whiteboard.\n",
    "\n",
    "AC1: Don’t display local user\n",
    "\n",
    "AC2: When remote user changes cursor location, display the change in animation.\n",
    "\n",
    "Provide an overall solution following the guidance mentioned above. Hint, keep all awareness information in a Konva layer, and an awareness info component to render cursor, and name. Don’t generate code. Describe the solution, and breaking the solution down as a task list based on the guidance mentioned above. And we will refer this task list as our master plan.\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
