{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# !pip install pysocks\n",
    "# %pip install chardet\n",
    "\n",
    "%reload_ext dotenv\n",
    "%dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/llmenv/lib/python3.11/site-packages/aiohttp/client_reqrep.py:70\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mcchardet\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mchardet\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:  \u001b[39m# pragma: no cover\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cchardet'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/ywu/ws/lrn/LLMs/llm_for_code/OpenAI-compatibles.ipynb 单元格 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/OpenAI-compatibles.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Config OpenAI client\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/OpenAI-compatibles.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/OpenAI-compatibles.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopenai\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/OpenAI-compatibles.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchat_models\u001b[39;00m \u001b[39mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/OpenAI-compatibles.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmanager\u001b[39;00m \u001b[39mimport\u001b[39;00m CallbackManager\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/llmenv/lib/python3.11/site-packages/openai/__init__.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39maiohttp\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[39mdel\u001b[39;00m sys\u001b[39m.\u001b[39mmodules[\u001b[39m\"\u001b[39m\u001b[39mpkg_resources\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mopenai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi_resources\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     20\u001b[0m     Audio,\n\u001b[1;32m     21\u001b[0m     ChatCompletion,\n\u001b[1;32m     22\u001b[0m     Completion,\n\u001b[1;32m     23\u001b[0m     Customer,\n\u001b[1;32m     24\u001b[0m     Deployment,\n\u001b[1;32m     25\u001b[0m     Edit,\n\u001b[1;32m     26\u001b[0m     Embedding,\n\u001b[1;32m     27\u001b[0m     Engine,\n\u001b[1;32m     28\u001b[0m     ErrorObject,\n\u001b[1;32m     29\u001b[0m     File,\n\u001b[1;32m     30\u001b[0m     FineTune,\n\u001b[1;32m     31\u001b[0m     FineTuningJob,\n\u001b[1;32m     32\u001b[0m     Image,\n\u001b[1;32m     33\u001b[0m     Model,\n\u001b[1;32m     34\u001b[0m     Moderation,\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mopenai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39merror\u001b[39;00m \u001b[39mimport\u001b[39;00m APIError, InvalidRequestError, OpenAIError\n\u001b[1;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mopenai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m VERSION\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/llmenv/lib/python3.11/site-packages/openai/api_resources/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mopenai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi_resources\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maudio\u001b[39;00m \u001b[39mimport\u001b[39;00m Audio  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mopenai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi_resources\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchat_completion\u001b[39;00m \u001b[39mimport\u001b[39;00m ChatCompletion  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mopenai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi_resources\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompletion\u001b[39;00m \u001b[39mimport\u001b[39;00m Completion  \u001b[39m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/llmenv/lib/python3.11/site-packages/openai/api_resources/audio.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, List\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopenai\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mopenai\u001b[39;00m \u001b[39mimport\u001b[39;00m api_requestor, util\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mopenai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi_resources\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mabstract\u001b[39;00m \u001b[39mimport\u001b[39;00m APIResource\n\u001b[1;32m      8\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mAudio\u001b[39;00m(APIResource):\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/llmenv/lib/python3.11/site-packages/openai/api_requestor.py:23\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     AsyncContextManager,\n\u001b[1;32m     12\u001b[0m     AsyncGenerator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     overload,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39murllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparse\u001b[39;00m \u001b[39mimport\u001b[39;00m urlencode, urlsplit, urlunsplit\n\u001b[0;32m---> 23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39maiohttp\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39mif\u001b[39;00m sys\u001b[39m.\u001b[39mversion_info \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/llmenv/lib/python3.11/site-packages/aiohttp/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Tuple\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m hdrs \u001b[39mas\u001b[39;00m hdrs\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     BaseConnector \u001b[39mas\u001b[39;00m BaseConnector,\n\u001b[1;32m      8\u001b[0m     ClientConnectionError \u001b[39mas\u001b[39;00m ClientConnectionError,\n\u001b[1;32m      9\u001b[0m     ClientConnectorCertificateError \u001b[39mas\u001b[39;00m ClientConnectorCertificateError,\n\u001b[1;32m     10\u001b[0m     ClientConnectorError \u001b[39mas\u001b[39;00m ClientConnectorError,\n\u001b[1;32m     11\u001b[0m     ClientConnectorSSLError \u001b[39mas\u001b[39;00m ClientConnectorSSLError,\n\u001b[1;32m     12\u001b[0m     ClientError \u001b[39mas\u001b[39;00m ClientError,\n\u001b[1;32m     13\u001b[0m     ClientHttpProxyError \u001b[39mas\u001b[39;00m ClientHttpProxyError,\n\u001b[1;32m     14\u001b[0m     ClientOSError \u001b[39mas\u001b[39;00m ClientOSError,\n\u001b[1;32m     15\u001b[0m     ClientPayloadError \u001b[39mas\u001b[39;00m ClientPayloadError,\n\u001b[1;32m     16\u001b[0m     ClientProxyConnectionError \u001b[39mas\u001b[39;00m ClientProxyConnectionError,\n\u001b[1;32m     17\u001b[0m     ClientRequest \u001b[39mas\u001b[39;00m ClientRequest,\n\u001b[1;32m     18\u001b[0m     ClientResponse \u001b[39mas\u001b[39;00m ClientResponse,\n\u001b[1;32m     19\u001b[0m     ClientResponseError \u001b[39mas\u001b[39;00m ClientResponseError,\n\u001b[1;32m     20\u001b[0m     ClientSession \u001b[39mas\u001b[39;00m ClientSession,\n\u001b[1;32m     21\u001b[0m     ClientSSLError \u001b[39mas\u001b[39;00m ClientSSLError,\n\u001b[1;32m     22\u001b[0m     ClientTimeout \u001b[39mas\u001b[39;00m ClientTimeout,\n\u001b[1;32m     23\u001b[0m     ClientWebSocketResponse \u001b[39mas\u001b[39;00m ClientWebSocketResponse,\n\u001b[1;32m     24\u001b[0m     ContentTypeError \u001b[39mas\u001b[39;00m ContentTypeError,\n\u001b[1;32m     25\u001b[0m     Fingerprint \u001b[39mas\u001b[39;00m Fingerprint,\n\u001b[1;32m     26\u001b[0m     InvalidURL \u001b[39mas\u001b[39;00m InvalidURL,\n\u001b[1;32m     27\u001b[0m     NamedPipeConnector \u001b[39mas\u001b[39;00m NamedPipeConnector,\n\u001b[1;32m     28\u001b[0m     RequestInfo \u001b[39mas\u001b[39;00m RequestInfo,\n\u001b[1;32m     29\u001b[0m     ServerConnectionError \u001b[39mas\u001b[39;00m ServerConnectionError,\n\u001b[1;32m     30\u001b[0m     ServerDisconnectedError \u001b[39mas\u001b[39;00m ServerDisconnectedError,\n\u001b[1;32m     31\u001b[0m     ServerFingerprintMismatch \u001b[39mas\u001b[39;00m ServerFingerprintMismatch,\n\u001b[1;32m     32\u001b[0m     ServerTimeoutError \u001b[39mas\u001b[39;00m ServerTimeoutError,\n\u001b[1;32m     33\u001b[0m     TCPConnector \u001b[39mas\u001b[39;00m TCPConnector,\n\u001b[1;32m     34\u001b[0m     TooManyRedirects \u001b[39mas\u001b[39;00m TooManyRedirects,\n\u001b[1;32m     35\u001b[0m     UnixConnector \u001b[39mas\u001b[39;00m UnixConnector,\n\u001b[1;32m     36\u001b[0m     WSServerHandshakeError \u001b[39mas\u001b[39;00m WSServerHandshakeError,\n\u001b[1;32m     37\u001b[0m     request \u001b[39mas\u001b[39;00m request,\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcookiejar\u001b[39;00m \u001b[39mimport\u001b[39;00m CookieJar \u001b[39mas\u001b[39;00m CookieJar, DummyCookieJar \u001b[39mas\u001b[39;00m DummyCookieJar\n\u001b[1;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mformdata\u001b[39;00m \u001b[39mimport\u001b[39;00m FormData \u001b[39mas\u001b[39;00m FormData\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/llmenv/lib/python3.11/site-packages/aiohttp/client.py:59\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mabc\u001b[39;00m \u001b[39mimport\u001b[39;00m AbstractCookieJar\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mclient_exceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     39\u001b[0m     ClientConnectionError \u001b[39mas\u001b[39;00m ClientConnectionError,\n\u001b[1;32m     40\u001b[0m     ClientConnectorCertificateError \u001b[39mas\u001b[39;00m ClientConnectorCertificateError,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     WSServerHandshakeError \u001b[39mas\u001b[39;00m WSServerHandshakeError,\n\u001b[1;32m     58\u001b[0m )\n\u001b[0;32m---> 59\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mclient_reqrep\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     60\u001b[0m     ClientRequest \u001b[39mas\u001b[39;00m ClientRequest,\n\u001b[1;32m     61\u001b[0m     ClientResponse \u001b[39mas\u001b[39;00m ClientResponse,\n\u001b[1;32m     62\u001b[0m     Fingerprint \u001b[39mas\u001b[39;00m Fingerprint,\n\u001b[1;32m     63\u001b[0m     RequestInfo \u001b[39mas\u001b[39;00m RequestInfo,\n\u001b[1;32m     64\u001b[0m     _merge_ssl_params,\n\u001b[1;32m     65\u001b[0m )\n\u001b[1;32m     66\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mclient_ws\u001b[39;00m \u001b[39mimport\u001b[39;00m ClientWebSocketResponse \u001b[39mas\u001b[39;00m ClientWebSocketResponse\n\u001b[1;32m     67\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconnector\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     68\u001b[0m     BaseConnector \u001b[39mas\u001b[39;00m BaseConnector,\n\u001b[1;32m     69\u001b[0m     NamedPipeConnector \u001b[39mas\u001b[39;00m NamedPipeConnector,\n\u001b[1;32m     70\u001b[0m     TCPConnector \u001b[39mas\u001b[39;00m TCPConnector,\n\u001b[1;32m     71\u001b[0m     UnixConnector \u001b[39mas\u001b[39;00m UnixConnector,\n\u001b[1;32m     72\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/llmenv/lib/python3.11/site-packages/aiohttp/client_reqrep.py:72\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mcchardet\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mchardet\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mcharset_normalizer\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mchardet\u001b[39;00m  \u001b[39m# type: ignore[no-redef]\u001b[39;00m\n\u001b[1;32m     75\u001b[0m __all__ \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mClientRequest\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mClientResponse\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRequestInfo\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFingerprint\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[39mif\u001b[39;00m TYPE_CHECKING:  \u001b[39m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/llmenv/lib/python3.11/site-packages/charset_normalizer/__init__.py:23\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mCharset-Normalizer\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39m:license: MIT, see LICENSE for more details.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcharset_normalizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m from_fp, from_path, from_bytes, normalize\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcharset_normalizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlegacy\u001b[39;00m \u001b[39mimport\u001b[39;00m detect\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcharset_normalizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__, VERSION\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-latest/envs/llmenv/lib/python3.11/site-packages/charset_normalizer/api.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m     PathLike \u001b[39m=\u001b[39m Union[\u001b[39mstr\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mos.PathLike[str]\u001b[39m\u001b[39m'\u001b[39m]  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcharset_normalizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconstant\u001b[39;00m \u001b[39mimport\u001b[39;00m TOO_SMALL_SEQUENCE, TOO_BIG_SEQUENCE, IANA_SUPPORTED\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcharset_normalizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmd\u001b[39;00m \u001b[39mimport\u001b[39;00m mess_ratio\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcharset_normalizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m CharsetMatches, CharsetMatch\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwarnings\u001b[39;00m \u001b[39mimport\u001b[39;00m warn\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "# Config OpenAI client\n",
    "import os\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "print('Check env -- PPLX endpoint:', os.getenv(\"PPLX_API_ENDPOINT\"), \\\n",
    "    'local endpoint:', os.getenv(\"LOCAL_API_ENDPOINT\"), \\\n",
    "    'proxy:', os.getenv('HTTP_PROXY'))\n",
    "\n",
    "# Perplexity https://docs.perplexity.ai/reference/post_chat_completions\n",
    "openai.api_key = os.getenv(\"PPLX_API_KEY\")\n",
    "openai.api_base = os.getenv(\"PPLX_API_ENDPOINT\")\n",
    "model_name = \"mistral-7b-instruct\" # llama-2-70b-chat, llama-2-13b-chat, codellama-34b-instruct, and mistral-7b-instruct.\n",
    "\n",
    "# Local LLMs\n",
    "# openai.api_key = os.getenv(\"LOCAL_API_KEY\")\n",
    "# openai.api_base = os.getenv(\"LOCAL_API_ENDPOINT\")\n",
    "# model_name = \"\"\n",
    "\n",
    "# Azure\n",
    "# openai.api_type = 'azure'\n",
    "# openai.api_version = '2023-05-15' # this may change in the future\n",
    "# model_name = ''\n",
    "\n",
    "# os.environ[\"OPENAI_API_TYPE\"] = openai.api_type\n",
    "# os.environ[\"OPENAI_API_BASE\"] = openai.api_base\n",
    "# os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
    "# os.environ[\"OPENAI_API_VERSION\"] = openai.api_version\n",
    "\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=openai.api_key,\n",
    "    openai_api_base=openai.api_base,\n",
    "    model=model_name,\n",
    "    max_tokens=500,\n",
    "    temperature=0.7,\n",
    "    verbose=True,\n",
    "    streaming=True,\n",
    "    callback_manager=callback_manager,\n",
    ")\n",
    "openai.debug = True\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are an artificial intelligence assistant and you need to \"\n",
    "            \"engage in a helpful, detailed, polite conversation with a user.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            \"Count to 100, with a comma between each number and no newlines. \"\n",
    "            \"E.g., 1, 2, 3, ...\"\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "# demo chat completion with streaming\n",
    "# response_stream = openai.ChatCompletion.create(\n",
    "#     api_base=openai.api_base,\n",
    "#     api_key=openai.api_key,\n",
    "#     model=model_name,\n",
    "#     messages=messages,\n",
    "#     stream=True,\n",
    "# )\n",
    "# for response in response_stream:\n",
    "#     print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm happy to assist you in any way I can. Before we begin, I just wanted to let you know that I'll be counting to 100 in this conversation. It's just a little something to make our chat more interesting and engaging. So, without further ado, let's get started!\n",
      "\n",
      "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100!\n",
      "\n",
      "I hope that was helpful and fun for you! Is there anything else I can assist you with?"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm happy to assist you in any way I can. Before we begin, I just wanted to let you know that I'll be counting to 100 in this conversation. It's just a little something to make our chat more interesting and engaging. So, without further ado, let's get started!\\n\\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100!\\n\\nI hope that was helpful and fun for you! Is there anything else I can assist you with?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(\"You are an artificial intelligence assistant and you need to \\\n",
    "            engage in a helpful, detailed, polite conversation with a user.\\\n",
    "            Count to 100, with a comma between each number and no newlines. \\\n",
    "            E.g., 1, 2, 3, ...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```plantuml\n",
    "@startuml\n",
    "\n",
    "start\n",
    "\n",
    "if (Graphviz installed?) then (yes)\n",
    "  :process all\\ndiagrams;\n",
    "else (no)\n",
    "  :process only\n",
    "  __sequence__ and __activity__ diagrams;\n",
    "endif\n",
    "\n",
    "stop\n",
    "\n",
    "@enduml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This PlantUML snippet defines a simple process that checks whether Graphviz is installed on the system, and based on the result, it executed a different set of diagrams.\n",
      "\n",
      "Here's a breakdown of the snippet:\n",
      "\n",
      "* `@startuml` and `@enduml` delimit the start and end of the PlantUML code.\n",
      "* `start` defines a new process and marks the beginning of the process.\n",
      "* `if (Graphviz installed?) then (yes)` checks whether Graphviz is installed on the system. The `if` statement is defined using the `if` keyword, followed by a pair of parentheses that contain the condition. In this case, the condition is `Graphviz installed?`, which is a built-in PlantUML feature that checks whether Graphviz is installed on the system. The `then` keyword is used to define the actions that should be taken if the condition is true.\n",
      "* `(:process all diagrams)` defines a new process that includes all diagrams. The `process` keyword is followed by a pair of parentheses that contain the name of the process. In this case, the process is named `all diagrams`.\n",
      "* `else (no)` defines an alternative branch of the `if` statement. If the condition `Graphviz installed?` is false, then this branch will be executed.\n",
      "* `(:process only __sequence__ and __activity__ diagrams)` defines a new process that includes only the sequence and activity diagrams. The `only` keyword is used to specify that only these two types of diagrams should be included in the process.\n",
      "* `endif` marks the end of the `if` statement.\n",
      "* `stop` defines a new process that marks the end of the process.\n",
      "\n",
      "In summary, this PlantUML snippet checks whether Graphviz is installed on the system, and if it is, it generates all diagrams, otherwise, it generates only sequence and activity diagrams."
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"This PlantUML snippet defines a simple process that checks whether Graphviz is installed on the system, and based on the result, it executed a different set of diagrams.\\n\\nHere's a breakdown of the snippet:\\n\\n* `@startuml` and `@enduml` delimit the start and end of the PlantUML code.\\n* `start` defines a new process and marks the beginning of the process.\\n* `if (Graphviz installed?) then (yes)` checks whether Graphviz is installed on the system. The `if` statement is defined using the `if` keyword, followed by a pair of parentheses that contain the condition. In this case, the condition is `Graphviz installed?`, which is a built-in PlantUML feature that checks whether Graphviz is installed on the system. The `then` keyword is used to define the actions that should be taken if the condition is true.\\n* `(:process all diagrams)` defines a new process that includes all diagrams. The `process` keyword is followed by a pair of parentheses that contain the name of the process. In this case, the process is named `all diagrams`.\\n* `else (no)` defines an alternative branch of the `if` statement. If the condition `Graphviz installed?` is false, then this branch will be executed.\\n* `(:process only __sequence__ and __activity__ diagrams)` defines a new process that includes only the sequence and activity diagrams. The `only` keyword is used to specify that only these two types of diagrams should be included in the process.\\n* `endif` marks the end of the `if` statement.\\n* `stop` defines a new process that marks the end of the process.\\n\\nIn summary, this PlantUML snippet checks whether Graphviz is installed on the system, and if it is, it generates all diagrams, otherwise, it generates only sequence and activity diagrams.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(\"\"\"Explain the following PlantUML snippet delimited by ```.\n",
    "\n",
    "```\n",
    "@startuml\n",
    "\n",
    "start\n",
    "\n",
    "if (Graphviz installed?) then (yes)\n",
    "  :process all\\ndiagrams;\n",
    "else (no)\n",
    "  :process only\n",
    "  __sequence__ and __activity__ diagrams;\n",
    "endif\n",
    "\n",
    "stop\n",
    "\n",
    "@enduml\n",
    "```\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I can help you with that! Here are some questions that can help determine which decision branch to follow based on the current system setup:\n",
      "\n",
      "1. Is Graphviz installed on the system?\n",
      "2. Are there any sequence diagrams or activity diagrams that need to be processed?\n",
      "3. Are there any other types of diagrams that need to be processed, such as class diagrams, use case diagrams, or state machine diagrams?\n",
      "4. Are there any specific requirements or constraints for the processing of the diagrams, such as formatting rules or output file requirements?\n",
      "5. Are there any other tools or software available for processing the diagrams, such as Pyreverse or UMLet?\n",
      "\n",
      "Answering these questions can help determine which branch of the decision tree to follow, based on the current system setup and requirements."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sure, I can help you with that! Here are some questions that can help determine which decision branch to follow based on the current system setup:\\n\\n1. Is Graphviz installed on the system?\\n2. Are there any sequence diagrams or activity diagrams that need to be processed?\\n3. Are there any other types of diagrams that need to be processed, such as class diagrams, use case diagrams, or state machine diagrams?\\n4. Are there any specific requirements or constraints for the processing of the diagrams, such as formatting rules or output file requirements?\\n5. Are there any other tools or software available for processing the diagrams, such as Pyreverse or UMLet?\\n\\nAnswering these questions can help determine which branch of the decision tree to follow, based on the current system setup and requirements.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(\"\"\"Based on the following PlantUML snippet delimited by ```, \\\n",
    "please ask some questions that would determine which decision branch we should follow \\\n",
    "based on the current system setup.\n",
    "\n",
    "```\n",
    "@startuml\n",
    "\n",
    "start\n",
    "\n",
    "if (Graphviz installed?) then (yes)\n",
    "  :process all\\ndiagrams;\n",
    "else (no)\n",
    "  :process only\n",
    "  __sequence__ and __activity__ diagrams;\n",
    "endif\n",
    "\n",
    "stop\n",
    "\n",
    "@enduml\n",
    "```\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: Missing dependencies for SOCKS support..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: Missing dependencies for SOCKS support..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: Missing dependencies for SOCKS support..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: Missing dependencies for SOCKS support..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ywu/ws/lrn/LLMs/llm_for_code/OpenAI-compatibles.ipynb 单元格 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/OpenAI-compatibles.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m llm\u001b[39m.\u001b[39;49mpredict(\u001b[39m\"\u001b[39;49m\u001b[39mPlease extract key information and takeaways from this link \u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/OpenAI-compatibles.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m  https://martinfowler.com/articles/2023-chatgpt-tech-writing.html, \u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ywu/ws/lrn/LLMs/llm_for_code/OpenAI-compatibles.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m  please generate the summary in the style of getAbstract.com.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SA-AI-PoC-_n3vcAnI/lib/python3.11/site-packages/langchain/chat_models/base.py:587\u001b[0m, in \u001b[0;36mBaseChatModel.predict\u001b[0;34m(self, text, stop, **kwargs)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    586\u001b[0m     _stop \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(stop)\n\u001b[0;32m--> 587\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m([HumanMessage(content\u001b[39m=\u001b[39;49mtext)], stop\u001b[39m=\u001b[39;49m_stop, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    588\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SA-AI-PoC-_n3vcAnI/lib/python3.11/site-packages/langchain/chat_models/base.py:551\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    545\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    546\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    550\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[0;32m--> 551\u001b[0m     generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    552\u001b[0m         [messages], stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    553\u001b[0m     )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    554\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m    555\u001b[0m         \u001b[39mreturn\u001b[39;00m generation\u001b[39m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SA-AI-PoC-_n3vcAnI/lib/python3.11/site-packages/langchain/chat_models/base.py:309\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    308\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 309\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    310\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    311\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[1;32m    312\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    313\u001b[0m ]\n\u001b[1;32m    314\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SA-AI-PoC-_n3vcAnI/lib/python3.11/site-packages/langchain/chat_models/base.py:299\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    297\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 299\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    300\u001b[0m                 m,\n\u001b[1;32m    301\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    302\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    303\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    304\u001b[0m             )\n\u001b[1;32m    305\u001b[0m         )\n\u001b[1;32m    306\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    307\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SA-AI-PoC-_n3vcAnI/lib/python3.11/site-packages/langchain/chat_models/base.py:446\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    443\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    445\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 446\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    447\u001b[0m         messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    448\u001b[0m     )\n\u001b[1;32m    449\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SA-AI-PoC-_n3vcAnI/lib/python3.11/site-packages/langchain/chat_models/openai.py:333\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[39mif\u001b[39;00m stream \u001b[39mif\u001b[39;00m stream \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstreaming:\n\u001b[1;32m    332\u001b[0m     generation: Optional[ChatGenerationChunk] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream(\n\u001b[1;32m    334\u001b[0m         messages\u001b[39m=\u001b[39mmessages, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    335\u001b[0m     ):\n\u001b[1;32m    336\u001b[0m         \u001b[39mif\u001b[39;00m generation \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    337\u001b[0m             generation \u001b[39m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SA-AI-PoC-_n3vcAnI/lib/python3.11/site-packages/langchain/chat_models/openai.py:305\u001b[0m, in \u001b[0;36mChatOpenAI._stream\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m}\n\u001b[1;32m    304\u001b[0m default_chunk_class \u001b[39m=\u001b[39m AIMessageChunk\n\u001b[0;32m--> 305\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompletion_with_retry(\n\u001b[1;32m    306\u001b[0m     messages\u001b[39m=\u001b[39;49mmessage_dicts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    307\u001b[0m ):\n\u001b[1;32m    308\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(chunk[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    309\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SA-AI-PoC-_n3vcAnI/lib/python3.11/site-packages/langchain/chat_models/openai.py:278\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    275\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 278\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SA-AI-PoC-_n3vcAnI/lib/python3.11/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SA-AI-PoC-_n3vcAnI/lib/python3.11/site-packages/tenacity/__init__.py:389\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoSleep):\n\u001b[1;32m    388\u001b[0m     retry_state\u001b[39m.\u001b[39mprepare_for_next_attempt()\n\u001b[0;32m--> 389\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msleep(do)\n\u001b[1;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     \u001b[39mreturn\u001b[39;00m do\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/SA-AI-PoC-_n3vcAnI/lib/python3.11/site-packages/tenacity/nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(seconds)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msleep\u001b[39m(seconds: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[39m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     time\u001b[39m.\u001b[39;49msleep(seconds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "llm.predict(\"Please extract key information and takeaways from this link \\\n",
    "  https://martinfowler.com/articles/2023-chatgpt-tech-writing.html, \\\n",
    "  please generate the summary in the style of getAbstract.com.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
